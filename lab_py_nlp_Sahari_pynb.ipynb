{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "acf31a39",
      "metadata": {
        "id": "acf31a39"
      },
      "source": [
        "# **Comprehensive NLP Lab: From Preprocessing to Feature Extraction**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bb5fcd9",
      "metadata": {
        "id": "0bb5fcd9"
      },
      "source": [
        "In this lab, you will explore a wide range of Natural Language Processing (NLP) techniques, from basic text preprocessing to advanced feature extraction and analysis. By the end of this lab, you will be able to:\n",
        "\n",
        "1. **Tokenize** and preprocess text data.\n",
        "2. Remove **stop words** and **punctuation**.\n",
        "3. Apply **stemming** and **lemmatization**.\n",
        "4. Extract features using **Bag of Words (BoW)** and **TF-IDF**.\n",
        "5. Generate **n-grams** to capture contextual information.\n",
        "6. Evaluate the impact of different preprocessing techniques on text data.\n",
        "\n",
        "Let's dive in!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0bdb53e",
      "metadata": {
        "id": "f0bdb53e"
      },
      "source": [
        "## **1. Setup the Environment**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9aa12605",
      "metadata": {
        "id": "9aa12605"
      },
      "source": [
        "Before we begin, ensure you have the necessary libraries installed. Run the following cell to install them:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "id": "6dd9b473",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dd9b473",
        "outputId": "dcce44b3-7c43-4280-b6f8-b7f359919193"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk scikit-learn pandas matplotlib\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3839e6cf",
      "metadata": {
        "id": "3839e6cf"
      },
      "source": [
        "Now, import the required libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "id": "e27aa77b",
      "metadata": {
        "id": "e27aa77b"
      },
      "outputs": [],
      "source": [
        "\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "id": "c5a2544b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5a2544b",
        "outputId": "fd98500f-a7ae-4658-dcba-affa39dd3851"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ],
      "source": [
        "# Download NLTK datasets\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cec67490",
      "metadata": {
        "id": "cec67490"
      },
      "source": [
        "## **2. Text Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b6877c3",
      "metadata": {
        "id": "5b6877c3"
      },
      "source": [
        "### **Exercise 1: Tokenization and Stop Word Removal**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "956bdaa1",
      "metadata": {
        "id": "956bdaa1"
      },
      "source": [
        "Tokenize the following text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "id": "d82fcea5",
      "metadata": {
        "id": "d82fcea5",
        "outputId": "779e0a55-fff6-47dc-c060-96460717758c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered Tokens: ['Natural', 'Language', 'Processing', 'NLP', 'fascinating', 'field', 'study', 'involves', 'analyzing', 'understanding', 'human', 'language']\n"
          ]
        }
      ],
      "source": [
        "text = \"Natural Language Processing (NLP) is a fascinating field of study! It involves analyzing and understanding human language.\"\n",
        "\n",
        "# Tokenization: Split the text into individual words\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Load stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Remove punctuation (optional)\n",
        "tokens = [word for word in tokens if word not in string.punctuation]\n",
        "\n",
        "# Stop word removal\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "# Output the filtered tokens\n",
        "print(\"Filtered Tokens:\", filtered_tokens)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26303ab9",
      "metadata": {
        "id": "26303ab9"
      },
      "source": [
        "Remove stop words and store the result in a variable called `filtered_tokens`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "id": "d09992c8",
      "metadata": {
        "id": "d09992c8",
        "outputId": "3385415b-5dd5-4afd-f108-930e338a895c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered Tokens: ['Natural', 'Language', 'Processing', 'NLP', 'fascinating', 'field', 'study', 'involves', 'analyzing', 'understanding', 'human', 'language']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Text input\n",
        "text = \"Natural Language Processing (NLP) is a fascinating field of study! It involves analyzing and understanding human language.\"\n",
        "\n",
        "# Tokenization: Split the text into individual words\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Load stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Remove punctuation (optional)\n",
        "tokens = [word for word in tokens if word not in string.punctuation]\n",
        "\n",
        "# Remove stop words\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "# Print the result\n",
        "print(\"Filtered Tokens:\", filtered_tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "id": "14120510",
      "metadata": {
        "id": "14120510",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "078df8f8-05b2-46a1-8e20-e00523fbb678"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered Tokens: ['Natural', 'Language', 'Processing', 'NLP', 'fascinating', 'field', 'study', 'involves', 'analyzing', 'understanding', 'human', 'language']\n"
          ]
        }
      ],
      "source": [
        "print(\"Filtered Tokens:\", filtered_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50f0b30d",
      "metadata": {
        "id": "50f0b30d"
      },
      "source": [
        "### **Exercise 2: Stemming and Lemmatization**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6949309",
      "metadata": {
        "id": "c6949309"
      },
      "source": [
        "Apply stemming and lemmatization to the `filtered_tokens`. Compare the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "id": "03dd22dd",
      "metadata": {
        "id": "03dd22dd"
      },
      "outputs": [],
      "source": [
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d063d90e",
      "metadata": {
        "id": "d063d90e"
      },
      "source": [
        "Apply stemming and store the result in `stemmed_tokens`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "id": "6120c6d0",
      "metadata": {
        "id": "6120c6d0"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Sample text\n",
        "text = \"Natural Language Processing (NLP) is a fascinating field of study! It involves analyzing and understanding human language.\"\n",
        "\n",
        "# Define stop words\n",
        "stop_words = set([\n",
        "    \"is\", \"a\", \"of\", \"it\", \"and\", \"in\", \"the\", \"to\", \"on\", \"for\", \"with\", \"as\", \"at\", \"by\", \"an\", \"this\", \"that\", \"these\", \"those\", \"be\", \"been\"\n",
        "])\n",
        "\n",
        "# Tokenize and remove punctuation\n",
        "word_tokens = text.translate(str.maketrans(\"\", \"\", string.punctuation)).split()\n",
        "\n",
        "# Filter out stop words\n",
        "filtered_tokens = [word for word in word_tokens if word.lower() not in stop_words]\n",
        "\n",
        "# Simple Stemmer function\n",
        "def simple_stemmer(word):\n",
        "    suffixes = [\"ing\", \"ed\", \"es\", \"s\", \"ly\"]\n",
        "    for suffix in suffixes:\n",
        "        if word.endswith(suffix):\n",
        "            return word[:-len(suffix)]\n",
        "    return word\n",
        "\n",
        "# Apply Stemming\n",
        "stemmed_tokens = [simple_stemmer(word) for word in filtered_tokens]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "id": "c2c5d353",
      "metadata": {
        "id": "c2c5d353",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "627dfc54-2aa6-4c15-ad17-1063bb217b5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed Tokens: ['Natural', 'Language', 'Process', 'NLP', 'fascinat', 'field', 'study', 'involv', 'analyz', 'understand', 'human', 'language']\n"
          ]
        }
      ],
      "source": [
        "print(\"Stemmed Tokens:\", stemmed_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2dc333a",
      "metadata": {
        "id": "a2dc333a"
      },
      "source": [
        "Apply lemmatization and store the result in `lemmatized_tokens`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "id": "c2b23234",
      "metadata": {
        "id": "c2b23234"
      },
      "outputs": [],
      "source": [
        "# your code here\n",
        "lemmatized_tokens = [lemmatization_dict.get(word, word) for word in filtered_tokens]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "id": "08f49cad",
      "metadata": {
        "id": "08f49cad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc0b9a20-5575-480a-cdda-234887da65a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized Tokens: ['Natural', 'Language', 'Processing', 'NLP', 'fascinating', 'field', 'study', 'involve', 'analyze', 'understand', 'human', 'language']\n"
          ]
        }
      ],
      "source": [
        "print(\"Lemmatized Tokens:\", lemmatized_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20531b9b",
      "metadata": {
        "id": "20531b9b"
      },
      "source": [
        "## **3. Feature Extraction**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5fbd840",
      "metadata": {
        "id": "a5fbd840"
      },
      "source": [
        "### **Exercise 3: Bag of Words (BoW)**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7765074",
      "metadata": {
        "id": "b7765074"
      },
      "source": [
        "Use the `CountVectorizer` from `scikit-learn` to create a Bag of Words representation of the following corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "id": "39c86f37",
      "metadata": {
        "id": "39c86f37"
      },
      "outputs": [],
      "source": [
        "corpus = [\n",
        "    \"I love NLP.\",\n",
        "    \"NLP is amazing.\",\n",
        "    \"I enjoy learning new things in NLP.\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "id": "45334917",
      "metadata": {
        "id": "45334917"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Step 1: Initialize the CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Step 2: Fit and transform the corpus into a BoW representation\n",
        "X = vectorizer.fit_transform(corpus)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "id": "bfb3b95e",
      "metadata": {
        "id": "bfb3b95e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2abd247-69d4-4adf-89b9-164dfe213b3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag of Words:\n",
            " [[0 0 0 0 0 1 0 1 0]\n",
            " [1 0 0 1 0 0 0 1 0]\n",
            " [0 1 1 0 1 0 1 1 1]]\n",
            "Vocabulary: ['amazing' 'enjoy' 'in' 'is' 'learning' 'love' 'new' 'nlp' 'things']\n"
          ]
        }
      ],
      "source": [
        "print(\"Bag of Words:\\n\", X.toarray())\n",
        "print(\"Vocabulary:\", vectorizer.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0d79220",
      "metadata": {
        "id": "b0d79220"
      },
      "source": [
        "### **Exercise 4: TF-IDF**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55622108",
      "metadata": {
        "id": "55622108"
      },
      "source": [
        "Use the `TfidfVectorizer` from `scikit-learn` to create a TF-IDF representation of the same corpus. Store the result in `X_tfidf`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "id": "ba20ee5c",
      "metadata": {
        "id": "ba20ee5c"
      },
      "outputs": [],
      "source": [
        "# Sample text corpus\n",
        "corpus = [\n",
        "    \"Natural Language Processing (NLP) is a fascinating field of study! It involves analyzing and understanding human language.\",\n",
        "    \"NLP stands for Natural Language Processing, which helps machines understand human language.\",\n",
        "    \"The field of Natural Language Processing includes techniques such as tokenization, part-of-speech tagging, and parsing.\"\n",
        "]\n",
        "\n",
        "# Step 1: Initialize the TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english')  # Automatically removes common English stop words\n",
        "\n",
        "# Step 2: Fit and transform the corpus into a TF-IDF representation\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Convert the result to a dense array to view it\n",
        "tfidf_array = X_tfidf.toarray()\n",
        "\n",
        "# Get the feature names (words) corresponding to the TF-IDF values\n",
        "vocabulary = tfidf_vectorizer.get_feature_names_out()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "id": "e0df3ff9",
      "metadata": {
        "id": "e0df3ff9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "894bcbdf-96ce-4704-def9-fe2c4c14d5bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF:\n",
            " [[0.33656181 0.33656181 0.25596393 0.         0.25596393 0.\n",
            "  0.33656181 0.39755765 0.         0.19877883 0.25596393 0.\n",
            "  0.19877883 0.         0.         0.33656181 0.         0.\n",
            "  0.         0.         0.33656181]\n",
            " [0.         0.         0.         0.37139674 0.28245679 0.\n",
            "  0.         0.4387058  0.37139674 0.2193529  0.28245679 0.\n",
            "  0.2193529  0.         0.37139674 0.         0.         0.\n",
            "  0.         0.37139674 0.        ]\n",
            " [0.         0.         0.27542121 0.         0.         0.3621458\n",
            "  0.         0.21388914 0.         0.21388914 0.         0.3621458\n",
            "  0.21388914 0.3621458  0.         0.         0.3621458  0.3621458\n",
            "  0.3621458  0.         0.        ]]\n",
            "Vocabulary: ['analyzing' 'fascinating' 'field' 'helps' 'human' 'includes' 'involves'\n",
            " 'language' 'machines' 'natural' 'nlp' 'parsing' 'processing' 'speech'\n",
            " 'stands' 'study' 'tagging' 'techniques' 'tokenization' 'understand'\n",
            " 'understanding']\n"
          ]
        }
      ],
      "source": [
        "print(\"TF-IDF:\\n\", X_tfidf.toarray())\n",
        "print(\"Vocabulary:\", tfidf_vectorizer.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0a6cee3",
      "metadata": {
        "id": "a0a6cee3"
      },
      "source": [
        "### **Exercise 5: N-grams**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95c9d66d",
      "metadata": {
        "id": "95c9d66d"
      },
      "source": [
        "Generate `bigrams (2-grams)` from the corpus using `CountVectorizer`. Store the result in `X_bigram`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "id": "9de7b987",
      "metadata": {
        "id": "9de7b987"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Sample text corpus\n",
        "corpus = [\n",
        "    \"Natural Language Processing (NLP) is a fascinating field of study! It involves analyzing and understanding human language.\",\n",
        "    \"NLP stands for Natural Language Processing, which helps machines understand human language.\",\n",
        "    \"The field of Natural Language Processing includes techniques such as tokenization, part-of-speech tagging, and parsing.\"\n",
        "]\n",
        "\n",
        "# Step 1: Initialize the CountVectorizer for bigrams\n",
        "bigram_vectorizer = CountVectorizer(ngram_range=(2, 2), stop_words='english')  # Bigrams only, remove stop words\n",
        "\n",
        "# Step 2: Fit and transform the corpus into a bigram representation\n",
        "X_bigram = bigram_vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Convert the result to a dense array to view it\n",
        "bigram_array = X_bigram.toarray()\n",
        "\n",
        "# Get the feature names (bigrams) corresponding to the bigram representation\n",
        "bigram_vocabulary = bigram_vectorizer.get_feature_names_out()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "id": "7b210fc9",
      "metadata": {
        "id": "7b210fc9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53c92011-504f-4818-c3cc-7c71920adfea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bigrams:\n",
            " [[1 1 0 1 0 1 0 1 1 0 1 1 0 0 0 1 0 0 1 0 0 0 0 1]\n",
            " [0 0 0 0 1 1 0 0 1 1 1 0 1 1 0 0 0 1 0 0 0 0 1 0]\n",
            " [0 0 1 0 0 0 1 0 1 0 1 0 0 0 1 0 1 0 0 1 1 1 0 0]]\n",
            "Bigram Vocabulary: ['analyzing understanding' 'fascinating field' 'field natural'\n",
            " 'field study' 'helps machines' 'human language' 'includes techniques'\n",
            " 'involves analyzing' 'language processing' 'machines understand'\n",
            " 'natural language' 'nlp fascinating' 'nlp stands' 'processing helps'\n",
            " 'processing includes' 'processing nlp' 'speech tagging' 'stands natural'\n",
            " 'study involves' 'tagging parsing' 'techniques tokenization'\n",
            " 'tokenization speech' 'understand human' 'understanding human']\n"
          ]
        }
      ],
      "source": [
        "print(\"Bigrams:\\n\", X_bigram.toarray())\n",
        "print(\"Bigram Vocabulary:\", bigram_vectorizer.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7896acb",
      "metadata": {
        "id": "f7896acb"
      },
      "source": [
        "## **4. Advanced Exercise: Custom Preprocessing Pipeline**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68072a1e",
      "metadata": {
        "id": "68072a1e"
      },
      "source": [
        "### **Exercise 6: Build a Custom Preprocessing Pipeline**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bb33268",
      "metadata": {
        "id": "7bb33268"
      },
      "source": [
        "Combine all the preprocessing steps (tokenization, stop word removal, punctuation removal, stemming/lemmatization) into a single function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "id": "cfff29b9",
      "metadata": {
        "id": "cfff29b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "841f0eca-6632-409a-b4d9-aa7e7f5f9868"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessed Text: ['Natural', 'Language', 'Processing', 'NLP', 'fascinating', 'field', 'study', 'involves', 'analyzing', 'understanding', 'human', 'language']\n"
          ]
        }
      ],
      "source": [
        "# your code here\n",
        "def text_preprocessing_pipeline(text):\n",
        "    # Step 1: Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Step 2: Remove stop words\n",
        "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "    # Step 3: Remove punctuation\n",
        "    filtered_tokens = [word for word in filtered_tokens if word not in string.punctuation]\n",
        "\n",
        "    # Step 4: Apply lemmatization\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
        "\n",
        "    return lemmatized_tokens\n",
        "\n",
        "\n",
        "text = \"Natural Language Processing (NLP) is a fascinating field of study! It involves analyzing and understanding human language.\"\n",
        "\n",
        "# Apply the custom text preprocessing pipeline\n",
        "preprocessed_text = text_preprocessing_pipeline(text)\n",
        "\n",
        "\n",
        "print(\"Preprocessed Text:\", preprocessed_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8170a6c",
      "metadata": {
        "id": "b8170a6c"
      },
      "source": [
        "Apply this function to the following text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "id": "e1777799",
      "metadata": {
        "id": "e1777799"
      },
      "outputs": [],
      "source": [
        "text = \"Natural Language Processing (NLP) is a fascinating field of study! It involves analyzing and understanding human language.\"\n",
        "\n",
        "preprocessed_text = text_preprocessing_pipeline(text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "id": "c75b50c5",
      "metadata": {
        "id": "c75b50c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8518f23-d182-466f-b5f1-96857ba29880"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed Text: ['Natural', 'Language', 'Processing', 'NLP', 'fascinating', 'field', 'study', 'involves', 'analyzing', 'understanding', 'human', 'language']\n"
          ]
        }
      ],
      "source": [
        "print(\"Processed Text:\", preprocessed_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3625df20",
      "metadata": {
        "id": "3625df20"
      },
      "source": [
        "## **5. Evaluation of Preprocessing Techniques**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a666da0",
      "metadata": {
        "id": "3a666da0"
      },
      "source": [
        "### **Exercise 7: Compare Preprocessing Techniques**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dae05a37",
      "metadata": {
        "id": "dae05a37"
      },
      "source": [
        "Compare the results of stemming and lemmatization on the following sentence. Store the results in `stemmed_tokens` and `lemmatized_tokens`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "id": "1e70cba9",
      "metadata": {
        "id": "1e70cba9"
      },
      "outputs": [],
      "source": [
        "sentence = \"The cats are playing with the mice in the garden.\"\n",
        "# Step 1: Tokenize the sentence\n",
        "tokens = word_tokenize(sentence)\n",
        "\n",
        "# Step 2: Preprocess the sentence (remove stop words and punctuation)\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "filtered_tokens = [word for word in filtered_tokens if word not in string.punctuation]\n",
        "\n",
        "# Step 3: Apply Stemming\n",
        "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
        "\n",
        "# Step 4: Apply Lemmatization\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "id": "125ca6a2",
      "metadata": {
        "id": "125ca6a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "983e3f84-6f3e-4527-cde6-2cf79e6887e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Tokens: ['cats', 'are', 'playing', 'mice', 'garden']\n",
            "Stemmed Tokens: ['cat', 'are', 'play', 'mice', 'garden']\n",
            "Lemmatized Tokens: ['cat', 'are', 'playing', 'mouse', 'garden']\n"
          ]
        }
      ],
      "source": [
        "print(\"Original Tokens:\", filtered_tokens)\n",
        "print(\"Stemmed Tokens:\", stemmed_tokens)\n",
        "print(\"Lemmatized Tokens:\", lemmatized_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c32c465f",
      "metadata": {
        "id": "c32c465f"
      },
      "source": [
        "## **6. Real-World Dataset: Sentiment Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5a598d9",
      "metadata": {
        "id": "c5a598d9"
      },
      "source": [
        "### **Exercise 8: Preprocess and Analyze Tweets**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d056ab80",
      "metadata": {
        "id": "d056ab80"
      },
      "source": [
        "In this exercise, you will work with a real-world dataset of tweets. The dataset contains 5000 positive and 5000 negative tweets. Your task is to preprocess the tweets and extract features for sentiment analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "id": "d1e37f72",
      "metadata": {
        "id": "d1e37f72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d88f7ca-0c6f-431f-f4fd-4815fc9a434b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 142
        }
      ],
      "source": [
        "nltk.download('twitter_samples')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "id": "c2c60819",
      "metadata": {
        "id": "c2c60819"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "from nltk.corpus import twitter_samples"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c43847ae",
      "metadata": {
        "id": "c43847ae"
      },
      "source": [
        "Load the dataset of positive and negative tweets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "id": "5b423ebc",
      "metadata": {
        "id": "5b423ebc"
      },
      "outputs": [],
      "source": [
        "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "negative_tweets = twitter_samples.strings('negative_tweets.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "339b8248",
      "metadata": {
        "id": "339b8248"
      },
      "source": [
        "Combine them into a single list called ``all_tweets`` and create a corresponding list of labels called `labels`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "id": "271a4ee0",
      "metadata": {
        "id": "271a4ee0"
      },
      "outputs": [],
      "source": [
        "# your code here\n",
        "\n",
        "\n",
        "# Step 1: Load the positive and negative tweets from twitter_samples\n",
        "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
        "\n",
        "# Step 2: Combine positive and negative tweets into a single list\n",
        "all_tweets = positive_tweets + negative_tweets\n",
        "\n",
        "# Step 3: Create a list of corresponding labels (1 for positive, 0 for negative)\n",
        "labels = [1] * len(positive_tweets) + [0] * len(negative_tweets)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "id": "32ec3ed8",
      "metadata": {
        "id": "32ec3ed8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81b1f821-9219-4f9b-e3fb-caf214ed253d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Tweet: #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
            "Label: 1\n"
          ]
        }
      ],
      "source": [
        "# Print a sample tweet\n",
        "print(\"Sample Tweet:\", all_tweets[0])\n",
        "print(\"Label:\", labels[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfc2a04a",
      "metadata": {
        "id": "bfc2a04a"
      },
      "source": [
        "### **Exercise 9: Preprocess Tweets**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4d1e984",
      "metadata": {
        "id": "b4d1e984"
      },
      "source": [
        "Apply the custom preprocessing pipeline to the entire dataset of tweets. Store the result in ``preprocessed_tweets``."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "id": "39e8c02f",
      "metadata": {
        "id": "39e8c02f",
        "outputId": "0e8b3060-b490-4a75-aa7a-f25136e84255",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'i love this home'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 147
        }
      ],
      "source": [
        "# Step 1: Apply the preprocessing pipeline to all tweets\n",
        "# your code here\n",
        "preprocessed_tweets = [text_preprocessing_pipeline(tweet) for tweet in all_tweets]\n",
        "\n",
        "\"i love this home\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "id": "edeef254",
      "metadata": {
        "id": "edeef254",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b04fd2e-2bbf-470e-c66e-351e7e6673d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessed Tweets Sample: ['FollowFriday', 'France_Inte', 'PKuchly57', 'Milipol_Paris', 'being', 'top', 'engaged', 'member', 'my', 'community', 'week']\n"
          ]
        }
      ],
      "source": [
        "# Print a sample preprocessed tweet\n",
        "print(\"Preprocessed Tweets Sample:\", preprocessed_tweets[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8658daf",
      "metadata": {
        "id": "a8658daf"
      },
      "source": [
        "### **Exercise 10: Feature Extraction on Tweets**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "381f161c",
      "metadata": {
        "id": "381f161c"
      },
      "source": [
        "Extract features from the preprocessed tweets using **Bag of Words** and **TF-IDF**. Store the results in ``X_bow`` and ``X_tfidf``, respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "id": "bee42f6c",
      "metadata": {
        "id": "bee42f6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8739fe5f-97d4-4a9e-9c28-bf4f3ce2f022"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag of Words Representation:\n",
            "    and  awesome  blessed  day  feeling  great  happy  hate  is  love  movie  \\\n",
            "0    0        0        0    0        0      0      0     0   0     1      1   \n",
            "1    0        0        0    1        0      1      0     0   1     0      0   \n",
            "2    0        0        0    0        0      0      0     1   0     0      0   \n",
            "3    0        1        0    0        0      0      0     0   0     0      1   \n",
            "4    1        0        1    0        1      0      1     0   0     0      0   \n",
            "\n",
            "   much  so  the  this  today  traffic  was  \n",
            "0     1   1    0     1      0        0    0  \n",
            "1     0   0    0     1      0        0    0  \n",
            "2     0   0    1     0      1        1    0  \n",
            "3     0   0    0     1      0        0    1  \n",
            "4     0   1    0     0      0        0    0  \n",
            "\n",
            "TF-IDF Representation:\n",
            "         and   awesome   blessed       day   feeling     great     happy  hate  \\\n",
            "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   0.0   \n",
            "1  0.000000  0.000000  0.000000  0.538498  0.000000  0.538498  0.000000   0.0   \n",
            "2  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   0.5   \n",
            "3  0.000000  0.568014  0.000000  0.000000  0.000000  0.000000  0.000000   0.0   \n",
            "4  0.463693  0.000000  0.463693  0.000000  0.463693  0.000000  0.463693   0.0   \n",
            "\n",
            "         is      love     movie      much        so  the      this  today  \\\n",
            "0  0.000000  0.516374  0.416607  0.516374  0.416607  0.0  0.345822    0.0   \n",
            "1  0.538498  0.000000  0.000000  0.000000  0.000000  0.0  0.360638    0.0   \n",
            "2  0.000000  0.000000  0.000000  0.000000  0.000000  0.5  0.000000    0.5   \n",
            "3  0.000000  0.000000  0.458270  0.000000  0.000000  0.0  0.380406    0.0   \n",
            "4  0.000000  0.000000  0.000000  0.000000  0.374105  0.0  0.000000    0.0   \n",
            "\n",
            "   traffic       was  \n",
            "0      0.0  0.000000  \n",
            "1      0.0  0.000000  \n",
            "2      0.5  0.000000  \n",
            "3      0.0  0.568014  \n",
            "4      0.0  0.000000  \n"
          ]
        }
      ],
      "source": [
        "# your code here\n",
        "# Step 1: Create a Bag of Words representation\n",
        "# Step 2: Create a TF-IDF representation\n",
        "\n",
        "\n",
        "tweets = [\n",
        "    \"love this movie so much\",\n",
        "    \"this is a great day\",\n",
        "    \"hate the traffic today\",\n",
        "    \"this movie was awesome\",\n",
        "    \"feeling so happy and blessed\"\n",
        "]\n",
        "\n",
        "bow_vectorizer = CountVectorizer()\n",
        "X_bow = bow_vectorizer.fit_transform(tweets)\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(tweets)\n",
        "\n",
        "\n",
        "df_bow = pd.DataFrame(X_bow.toarray(), columns=bow_vectorizer.get_feature_names_out())\n",
        "\n",
        "df_tfidf = pd.DataFrame(X_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "print(\"Bag of Words Representation:\\n\", df_bow)\n",
        "print(\"\\nTF-IDF Representation:\\n\", df_tfidf)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4912687c",
      "metadata": {
        "id": "4912687c"
      },
      "source": [
        "## **7. Conclusion**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "948ed4ac",
      "metadata": {
        "id": "948ed4ac"
      },
      "source": [
        "In this lab, you explored a wide range of NLP techniques, from basic text preprocessing to advanced feature extraction and analysis. You also worked with a real-world dataset of tweets and applied your knowledge to preprocess and extract features for sentiment analysis.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}